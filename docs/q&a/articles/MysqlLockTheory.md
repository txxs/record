[TOC]

---
实现数据库，首先要看是怎么实现它的特性的，也就是ACID，这是数据库的底子，是这些保障了数据库的平稳运行。在此基础上数据库的另外一个挑战就是数据库的并发，MVCC解决了快照读和写之间的并发问题，但对于写和写之间、当前读和写之间的并发，MVCC就无能为力了，这时就需要用到锁。再进一步，利用锁就可以实现不同的数据库的隔离级别。同时锁也是实现ACID中I（隔离性）的手段。

### 1. ACID 原理

先看一个结论性的内容：

- A和D：通过Undo Log + Redo Log实现事务的A（原子性）和D（持久性）。
- I：通过“MVCC + 锁”实现了事务的I（隔离性）和并发性。
- C：锁 + Bin Log + 代码，可以实现数据库的一致性。一致性是最基本的属性，其它的三个属性都为了保证一致性而存在的，像事务最重要的目的其实是为了满足一致性。例如，事务1需要将100元转入帐号A：先读取帐号A的值，然后在这个值上加上100。但是，在这两个操作之间，另一个事务2修改了帐号A的值，为它增加了100元。那么最后的结果应该是A增加了200元。但事实上，事务1最终完成后，帐号A只增加了100元，因为事务2的修改结果被事务1覆盖掉了。

> 引用自：https://mp.weixin.qq.com/s/2FESBGqNctINFKdWQJNM3g

#### 1.1 Redo log

##### 1.1.1 基础知识（时间不够可以不看）

> 引用自：http://catkang.github.io/2020/02/27/mysql-redo.html

为什么需要记录REDO：为了取得更好的读写性能，InnoDB会将数据缓存在内存中（InnoDB Buffer Pool），对磁盘数据的修改也会落后于内存，这时如果进程或机器崩溃，会导致内存数据丢失，为了保证数据库本身的一致性和持久性，InnoDB维护了REDO LOG。修改Page之前需要先将修改的内容记录到REDO中，并保证REDO LOG早于对应的Page落盘，也就是常说的Write Ahead Log。当故障发生导致内存数据丢失后，InnoDB会在重启时，通过重放REDO，将Page恢复到崩溃前的状态。


那么我们需要什么样的REDO呢？首先，REDO的维护增加了一份写盘数据，同时为了保证数据正确，事务只有在他的REDO全部落盘才能返回用户成功，REDO的写盘时间会直接影响系统吞吐，显而易见，REDO的数据量要尽量少。其次，系统崩溃总是发生在始料未及的时候，当重启重放REDO时，系统并不知道哪些REDO对应的Page已经落盘，因此REDO的重放必须可重入，即REDO操作要保证幂等。最后，为了便于通过并发重放的方式加快重启恢复速度，REDO应该是基于Page的，即一个REDO只涉及一个Page的修改。熟悉的读者会发现，数据量小是Logical Logging的优点，而幂等以及基于Page正是Physical Logging的优点，因此InnoDB采取了一种称为Physiological Logging的方式，来兼得二者的优势。所谓Physiological Logging，就是以Page为单位，但在Page内以逻辑的方式记录。举个例子，MLOG_REC_UPDATE_IN_PLACE类型的REDO中记录了对Page中一个Record的修改，方法如下：

```
（Page ID，Record Offset，(Filed 1, Value 1) … (Filed i, Value i) … )
```

其中，PageID指定要操作的Page页，Record Offset记录了Record在Page内的偏移位置，后面的Field数组，记录了需要修改的Field以及修改后的Value。由于Physiological Logging的方式采用了物理Page中的逻辑记法，导致两个问题：

- 需要基于正确的Page状态上重放REDO：由于在一个Page内，REDO是以逻辑的方式记录了前后两次的修改，因此重放REDO必须基于正确的Page状态。然而InnoDB默认的Page大小是16KB，是大于文件系统能保证原子的4KB大小的，因此可能出现Page内容成功一半的情况。InnoDB中采用了Double Write Buffer的方式来通过写两次的方式保证恢复的时候找到一个正确的Page状态。这部分会在之后介绍Buffer Pool的时候详细介绍。

- 需要保证REDO重放的幂等：Double Write Buffer能够保证找到一个正确的Page状态，我们还需要知道这个状态对应REDO上的哪个记录，来避免对Page的重复修改。为此，InnoDB给每个REDO记录一个全局唯一递增的标号LSN(Log Sequence Number)。Page在修改时，会将对应的REDO记录的LSN记录在Page上（FIL_PAGE_LSN字段），这样恢复重放REDO时，就可以来判断跳过已经应用的REDO，从而实现重放的幂等。


REDO是如何组织的

所谓REDO的组织方式，就是如何把需要的REDO内容记录到磁盘文件中，以方便高效的REDO写入，读取，恢复以及清理。我们这里把REDO从上到下分为三层：逻辑REDO层、物理REDO层和文件层。
- 逻辑REDO层，这一层是真正的REDO内容，REDO由多个不同Type的多个REDO记录收尾相连组成，有全局唯一的递增的偏移sn，InnoDB会在全局log_sys中维护当前sn的最大值，并在每次写入数据时将sn增加REDO内容长度。
- 物理REDO层，磁盘是块设备，InnoDB中也用Block的概念来读写数据，一个Block的长度OS_FILE_LOG_BLOCK_SIZE等于磁盘扇区的大小512B，每次IO读写的最小单位都是一个Block。
- 文件层，最终REDO会被写入到REDO日志文件中，以ib_logfile0、ib_logfile1…命名，为了避免创建文件及初始化空间带来的开销，InooDB的REDO文件会循环使用，通过参数innodb_log_files_in_group可以指定REDO文件的个数。多个文件收尾相连顺序写入REDO内容。每个文件以Block为单位划分，每个文件的开头固定预留4个Block来记录一些额外的信息，其中第一个Block称为Header Block，之后的3个Block在0号文件上用来存储Checkpoint信息，而在其他文件上留空。通过LSN可以唯一标识一个REDO位置，但最终对REDO的读写还需要转换到对文件的读写IO，这个时候就需要表示文件空间的offset。

> 引用自：https://segmentfault.com/a/1190000023827696

另外，我们都知道，事务的四大特性里面有一个是**持久性**，具体来说就是只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态。那么mysql是如何保证一致性的呢？最简单的做法是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。但是这么做会有严重的性能问题，主要体现在两个方面：

- 因为 Innodb 是以 页 为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！
- 一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差！

因此mysql设计了redo log，具体来说就是只记录事务对数据页做了哪些修改，这样就能完美地解决性能问题了(相对而言文件更小并且是顺序IO)。redo log 包括两部分：一个是内存中的日志缓冲( redo log buffer )，另一个是磁盘上的日志文件(redo log file )。mysql每执行一条DML语句，先将记录写入redo log buffer，后续某个时间点再一次性将多个操作记录写到redo log file。这种先写日志，再写磁盘的技术就是MySQL里经常说到的 WAL(Write-Ahead Logging) 技术。在计算机操作系统中，用户空间(user space)下的缓冲区数据一般情况下是无法直接写入磁盘的，中间必须经过操作系统内核空间(kernel space)缓冲区(OS Buffer)。因此，redo log buffer写入 redo log
file 实际上是先写入 OS Buffer ，然后再通过系统调用 fsync() 将其刷到redo log file中，过程如下：

![图片](images/WX20210823-192114@2-2x.png)

一个事务要修改多张表的多条记录，多条记录分布在不同的Page里面，对应到磁盘的不同位置。如果每个事务都直接写磁盘，一次事务提交就要多次磁盘的随机I/O，性能达不到要求。怎么办呢？不写磁盘，在内存中进行事务提交。然后再通过后台线程，异步地把内存中的数据写入到磁盘中。但有个问题：机器宕机，内存中的数据还没来得及刷盘，数据就丢失了。
为此，就有了Write-aheadLog的思路：先在内存中提交事务，然后写日志（所谓的Write-ahead Log），然后后台任务把内存中的数据异步刷到磁盘。日志是顺序地在尾部Append，从而也就避免了一个事务发生多次磁盘随机I/O的问题。明明是先在内存中提交事务，后写的日志，为什么叫作Write-Ahead呢？这里的Ahead，其实是指相对于真正的数据刷到磁盘，因为是先写的日志，后把内存数据刷到磁盘，所以叫Write-Ahead Log。
内存操作数据 +Write-Ahead Log的这种思想非常普遍，后面讲LSM树的时候，还会再次提到这个思想。在多备份一致性中，复制状态机的模型也是基于此。
具体到InnoDB中，Write-Ahead Log是Redo Log。在InnoDB中，不光事务修改的数据库表数据是异步刷盘的，连Redo Log的写入本身也是异步的。如图6-7所示，在事务提交之后，Redo Log先写入到内存中的Redo Log Buffer中，然后异步地刷到磁盘上的Redo Log。

为此，InnoDB有个关键的参数innodb_flush_log_at_trx_commit控制Redo Log的刷盘策略，该参数有三个取值：
- 0：每秒刷一次磁盘，把Redo Log Buffer中的数据刷到Redo Log（默认为0）。
- 1：每提交一个事务，就刷一次磁盘（这个最安全）。
- 2：不刷盘。然后根据参数innodb_flush_log_at_timeout设置的值决定刷盘频率。

![图片](images/WX20210823-192114@2-3x.png)

很显然，该参数设置为0或者2都可能丢失数据。设置为1最安全，但性能最差。InnoDB设置此参数，也是为了让应用在数据安全性和性能之间做一个权衡。

![图片](images/WX20210823-192114@2-1x.png)

同时我们很容易得知， 在innodb中，既有 redo log 需要刷盘，还有 数据页 也需要刷盘， redo log 存在的意义主要就是降低对 数据页 刷盘的要求 。在上图中， write pos 表示 redo log 当前记录的 LSN (逻辑序列号)位置， check point 表示 数据页更改记录** 刷盘后对应 redo log 所处的 LSN (逻辑序列号)位置。 write pos 到 check point 之间的部分是 redo log 空着的部分，用于记录新的记录； check point 到 write pos 之间是 redo log 待落盘的数据页更改记录。当 write pos 追上 check point 时，会先推动 check point 向前移动，空出位置再记录新的日志。

启动 innodb 的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。因为 redo log 记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志(如 binlog )要快很多。 重启 innodb 时，首先会检查磁盘中数据页的 LSN ，如果数据页的 LSN 小于日志中的 LSN ，则会从 checkpoint 开始恢复。 还有一种情况，在宕机前正处于
checkpoint 的刷盘过程，且数据页的刷盘进度超过了日志页的刷盘进度，此时会出现数据页中记录的 LSN 大于日志中的 LSN
，这时超出日志进度的部分将不会重做，因为这本身就表示已经做过的事情，无需再重做。


> 引用自：https://www.jianshu.com/p/336e4995b9b8

##### 1.1.2 Redo log写入流程

![图片](images/WX20210823-192114@2-4x.png)

上边面以一个更新事务为例，宏观执行流程：

1. 将原始数据从磁盘中读入内存中，修改数据在内存中的拷贝
2. 生成一条redo log并写入redo log buffer，记录的是数据被修改后的值
3. 当事务commit时，将redo log buffer中的内容刷新到 redo log file，对 redo log file采用追加写的方式
4. 定期将内存中修改的数据刷新到磁盘中

> 引用自，非常好： https://blog.csdn.net/huangzhilin2015/article/details/115396599

更为详细的过程：

![图片](images/WX20210823-192114@2-5x.png)

##### 1.1.3 Redo redo如何保证事务的持久性？

InnoDB是事务的存储引擎，其通过Force Log at Commit 机制实现事务的持久性。
即当事务提交时，先将 redo log buffer 写入到 redo log file 进行持久化，待事务的commit操作完成时才算完成。这种做法也被称为 Write-Ahead Log(预先日志持久化):在持久化一个数据页之前，先将内存中相应的日志页持久化。

为了保证每次日志都写入redo log file，在每次将redo buffer写入redo log file之后，默认情况下，InnoDB存储引擎都需要调用一次 fsync操作,因为redo log并没有 O_DIRECT选项，所以redo log先写入到文件系统缓存。为了确保redo log写入到磁盘，必须进行一次 fsync操作。fsync是一种系统调用操作，其fsync的效率取决于磁盘的性能，因此磁盘的性能也影响了事务提交的性能，也就是数据库的性能。

**也就是说redo log的作用是持久性ACID中的C**，但Redo log只是C的其中一部分，另外一部分的C是由Bin log实现的。

#### 1.2 undo log

数据库事务开始之前，会将要修改的记录存放到 Undo 日志里，当事务回滚时或者数据库崩溃时，可以利用 Undo 日志，撤销未提交事务对数据库产生的影响。

Undo Log产生和销毁：Undo Log在事务开始前产生；事务在提交时，并不会立刻删除undo log，innodb会将该事务对应的undo log放入到删除列表中，后面会通过后台线程purge thread进行回收处理。Undo Log属于逻辑日志，记录一个变化过程。例如执行一个delete，undolog会记录一个insert；执行一个update，undolog会记录一个相反的update。

Undo Log存储：Undo log采用段的方式管理和记录。在innodb数据文件中包含一种rollback segment回滚段，内部包含1024个undo log segment。可以通过下面一组参数来控制Undo log存储。

> 引用自：https://juejin.cn/post/6987557227074846733

#### 1.2.1 undo log 作用

- 保障事务的原子性。事务中的所有更新语句，要么全被执行，要么全被回滚。通过将同一个trx_id 的undo log 就可以实现回滚同一事务的所有更新语句了。
- 提供版本链，支持MVCC。undo log 链为实现MVCC 提供了底层的物质基础。为了实现MVCC ,还需要借助Read View 。

虽然undo log 和redo log 都是InnoDB 特有的，但**undo log记录的是逻辑日志，redo log记录的是物理日志**。对记录做变更操作时不仅会产生redo记录，也会产生undo记录（insert,update,delete），undo log 日志用于存放数据被修改前的值，比如 update T set c=c+1 where ID=2; 这条 SQL，undo log中记录的是c在 +1前的值，如果这个update出现异常需要回滚，可以使用undo log实现回滚，保证事务一致性。

> 引用自：https://blog.csdn.net/huangzhilin2015/article/details/115396599

#### 1.2.1 undo log存储

当一个事务对记录做出了变更，就会就会产生undo log，默认被记录到**系统表空间(ibdata)**中，在5.6之后的版本也可以使用独立的undo表空间。关于表空间，可以理解为磁盘上的物理文件，比如table1.ibd，就代表的table1表的独立表空间，其中包含了数据页、索引等数据。可以通过语句：

```
show variables like '%innodb_data_file_path%'
结果：ibdata1:12M:autoextend
```

查看系统表空间，结果显示格式为name:size:attributes，分别表示名称，大小和属性，autoextend表示其会随着数据增多自动扩容。相对于redo log是一种物理日志（记录了某个数据页发生了什么更改）来说，undo log则是一种逻辑日志，当一个事务对记录做了变更操作就会产生undo log，也就是说undo log记录了记录变更的逻辑过程。笼统的说，当一个事务要更新一行记录时，会把当前记录当做历史快照保存下来，多个历史快照会用两个隐藏字段trx_id和roll_pointer串起来，形成一个历史版本链，当需要事务回滚时，可以依赖这个历史版本链将记录回滚到事务开始之前的状态，从而保证了事务的原子性（一个事务对数据库的所有操作，要么全部成功，要么全部失败）。


![图片](images/WX20210823-192114@2-6x.png)

总结来说，在InnoDB里，undo log分为两种类型：

- insert undo log：插入产生的undo log。不需要维护历史版本链，因为没有历史数据，所以其产生的undo log可以在事务提交之后删除，不需要purge操作
- update undo log：更新或删除产生的undo log。不会在提交后就立即删除，而是会放入undo log历史版本链，用于MVCC，最后由purge线程清理

为了保证多个事务并发操作，在写各自的undo log时不产生冲突，InnoDB采用一种叫做回滚段（rollback segment，rseg）的结构来存储undolog，InnoDB中最多可以创建128个回滚段，每个回滚段维护了一个段头页，在该页中又划分了1024个slot，每个slot又对应到一个undo log对象。虽然回滚段最多可以有128个，但是对于回滚段的布局结构：

- rseg0：预留在系统表空间ibdata中
- rseg 1- rseg 32：这32个回滚段存放于临时表的系统表空间中
- rseg33 - 128： 普通回滚段，根据配置存放到独立的undo表空间中，或者如果没有打开独立undo表空间，则存放于系统表空间ibdata中

  所以理论上InnoDB最多支持 96 * 1024个普通事务。在每一个读写事务开始（或只读事务转变为读写事务）的时候，都会以轮询的方式预先为其分配一个普通回滚段（对于只读事务，如果产生对临时表的写入，则需要使用第1~32号临时表回滚段为其分配回滚段），回滚段分配之后，在这个事务的生命周期内，都会使用这个回滚段。当一个事务要存储undo log的时候，就会从这个事务所使用的回滚段的1024个slot中根据使用undo log类型（insert或update）分配一个slot，如果同一类型的slot之前已经分配过，那么可以直接使用，否则就需要分配一个slot，并创建一个对应的undo页，然后初始化。但是如果回滚段都用完了则会返回错误。在事务提交后，需要purge的回滚段会被放到purge队列上，留待后台purge线程清理。另外还需要注意一点，对于删除操作，InnoDB并不是真正的删除原来的记录，而是将其delete mark设置为1，purge线程会把这些标记未删除的数据真正从磁盘上删除。


> 引用自：https://segmentfault.com/a/1190000039180234

#### 1.2 undo log和Redo log恢复数据

由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。（会造成数据不一致）

当我们查询数据的时候，会先去Buffer Pool中查询。如果Buffer Pool中不存在，存储引擎会先将数据从磁盘加载到Buffer Pool中，然后将数据返回给客户端；同理，当我们更新某个数据的时候，如果这个数据不存在于Buffer Pool，同样会先数据加载进来，然后修改修改内存的数据。被修改过的数据会在之后统一刷入磁盘。

![图片](images/WX20210823-192114@2-7x.png)

假设我们修改Buffer Pool中的数据成功，但是还没来得及将数据刷入磁盘MySQL就挂了怎么办？按照上图的逻辑，此时更新之后的数据只存在于Buffer Pool中，如果此时MySQL宕机了，这部分数据将会永久的丢失；2而Redo Log和Undo Log，可以解决这个问题

- Redo Log 记录了此次事务 「完成后」 的数据状态，记录的是更新之 「后」 的值。
- Undo Log 记录了此次事务 「开始前」 的数据状态，记录的是更新之 「前」 的值。

具体流程是：

- 首先，更新数据还是会判断数据是否存在于Buffer Pool中，不存在则加载。上面我们提到了回滚的问题，在更新Buffer Pool中的数据之前，我们需要先将该数据事务开始之前的状态写入Undo Log中。假设更新到一半出错了，我们就可以通过Undo Log来回滚到事务开始前；
- 然后执行器会更新Buffer Pool中的数据，成功更新后会将数据最新状态写入Redo Log Buffer中。因为一个事务中可能涉及到多次读写操作，写入Buffer中分组写入，比起一条条的写入磁盘文件，效率会高很多。
- 那为什么Undo Log不也搞一个Undo Log Buffer，也给Undo Log提提速，雨露均沾？那我们假设有这个一个Buffer存在于InnoDB，将事务开始前的数据状态写入了Undo Log Buffer中，然后开始更新数据。如果MySQL由于意外进程退出了，你发现Undo Log随着进程退出一起没了，此时就没有办法通过Undo Log去做回滚。
- 那如果刚刚更新完内存，MySQL就挂了呢？此时Redo Log Buffer甚至都可能没有写入，即使写入了也没有刷到磁盘，Redo Log也丢了。其实无所谓，因为意外宕机，该事务没有成功，既然事务事务没有成功那就需要回滚，而MySQL重启后会读取磁盘上的Redo Log文件，将其状态给加载到Buffer Pool中。而通过磁盘Redo Log文件恢复的状态和宕机前事务开始前的状态是一样的，所以是没有影响的。**然后等待事务commit了之后就会将Redo Log和Binlog刷到磁盘**。

基于2PC的一致性保障：

从这你可以发现一个关键的问题，**那就是必须保证Redo Log和Binlog在事务提交时的数据一致性**，要么都存在，要么都不存在。MySQL是通过 2PC（two-phase commit protocol）来实现的。

![图片](images/WX20210823-192114@2-8x.png)

它是一种保证分布式事务数据一致性的协议，它中文名叫两阶段提交，它将分布式事务的提交拆分成了2个阶段，分别是Prepare和Commit/Rollback。

- Prepare阶段，将Redo Log写入文件，并刷入磁盘，记录上内部XA事务的ID，同时将Redo Log状态设置为Prepare。Redo Log写入成功后，再将Binlog同样刷入磁盘，记录XA事务ID。
- Commit阶段，向磁盘中的Redo Log写入Commit标识，表示事务提交。然后执行器调用存储引擎的接口提交事务。这就是整个过程。

协调器最终就是一个协调者的角色，最终会在Redolog中记录写入成功，如下图：

![图片](images/WX20210823-192114@2-9x.png)


MySQL最开始是没有InnoDB引擎的，binlog日志位于Server层，只是用于归档和主从复制，本身不具备crashsafe的能力。而InnoDB依靠redolog具备了crashsafe的能力，redolog和binlog同时记录，就需要保证两者的一致性。在前面小节中已经体现了，两个log的写入流程是：

> 写入relo log->事务状态设置为prepare->写入binlog->提交事务->修改redolog事务状态为commit

先prepare后commit，这个称为两段提交。那么为什么需要两个段提交呢？redo log和binlog是两种不同的日志，就类似于分布式中的多节点提交请求，需要保证事务的一致性。redo log和binlog有一个公共字段XID，代表事务ID。当参数innodb_support_xa打开时，在执行事务的第一条SQL时候会去注册XA，根据第一条SQL的query id拼凑XID数据，然后存储在事务对象中。

采取了两段提交之后，怎么做crash恢复呢？如果在写入binlog之前宕机了，那么事务需要回滚；如果事务commit之前宕机了，那么此时binlog cache中的数据可能还没有刷盘，那么验证binlog的完整性：到redo log中找到最近事务的XID，根据这个XID到binlog中去找（XID Event），如果找到了，说明在binlog中对应事务已经提交，那么提交redo log中事务即可；否则需要回滚事务。至于为什么有binlog和redo log的共存，导致了这么一个复杂的局面，前面也提到了，InnoDB是后面引进MySQL的，redo log属于InnoDB特有，保证了事务的持久性，而binlog则位于Server层，用于归档。


#### 1.3 Binlog

https://mp.weixin.qq.com/s/-CXTVPkUdMkT-6PB3lHLRw

MySQL的整体架构其实有两块：一块是Server层，还有一块是引擎层，负责存储相关。前面我们提到的redo log是InnoDB 引擎持有的，而Server层也有自己的日志，叫binlog（归档日志）。那为何会有两份日志呢？因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力（因为是Server层与引擎层是两个独立的模块），binlog 日志只能用于归档。而InnoDB是另一个公司以插件形式引入 MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。假如只binlog，当Server层binlog日志写完后引擎层还没有同步到磁盘就断电了。这个时候重启后binlog记录了更新操作，但是引擎层并没有写入磁盘中就导致了从库使用该binlog同步数据不一致。

> 引用自：https://blog.csdn.net/huangzhilin2015/article/details/115396599

redo log buffer主要可以在buffer pool数据还未刷盘宕机时保证事务的持久性。**而binlog主要用于主从复制和数据恢复**。可以通过参数max_binlog_size设置每个binlog文件的大小，新增binlog数据时直接向文件末尾添加，如果文件大小达到了参数配置值，那么数据会记录到新文件中，这个和redo log的环形日志有鲜明的对比。binlog日志有三种级别，可以通过binlog-format指定，分别是：

- statement：基于SQL语句的赋值。只记录SQL，不记录数据变更，日志文件比较小，能够节约网络和磁盘IO，但是准确性不高，对一些系统函数，比如now()，不能准确复制。
- row：基于行的变更。不记录SQL，记录每行实际数据的变更，准确性比较高，但是由于记录了数据变更，所以日志文件较大，相对于statement有更高的网络和磁盘IO，通常建议使用这个级别。
- mixed：基于statement和row的混合模式。默认使用statement，statement无法复制的操作则使用row。可能发生binlog丢失，导致主从不一致（还没去验证过）

在主从同步的场景中，Master开启了binlog日志之后，会根据binlog级别将对应的日志内容记录到二进制文件中。Slave上会启动IO线程连接到Master，请求读取指定日志文件指定位置的日志内容，Master接收到Slave的请求后，会有根据请求的日志文件和位置读取日志内容，然后返回给Slave，同时还会返回所读取日志文件现在到了哪个位置。Slave收到日志内容后，会将数据添加到relaylog文件的末尾，并且将Master返回的binlog文件和对应的最新位置记录到master info文件中，下次读取对应日志文件的时候就可以告诉Master从这个位置开始读取。Slave检测到realy log文件有新增后，会解析内容，如果日志基于statement，那么就在Slave上重新执行这些SQL，如果日志基于row，那么Slave直接根据日志内容对对应的行做修改。但是要注意的是，对于Master来说，binlog也不是每次直接写入磁盘的，binlog也有一个binlog cache，在事务提交之前，数据会放入binlog cache，提交之后再从binlog cache刷入磁盘。通过参数binlog_cache_size可以设置binlog cache的大小，通过参数sync_binlog控制刷盘策略：

- 0：不立即刷盘，由系统决定何时将binlog cache刷盘，性能最高，但是可能会丢失多个事务的数据
- N：每N个事务提交之后，将binlog cache刷盘，当N=1时，数据最安全，最多丢失一个事务的数据，但是性能也最低；当N大于1时，会累积多个事务，类似于0的情况，可能会丢失多个事务的数据

![图片](images/WX20210823-192114@2-10x.png)

> 引用自：http://www.jiangxinlingdu.com/mysql/2019/06/07/binlog.html

主动复制主要分为三个步骤，如如下图：

![图片](images/WX20210823-192114@2-11x.png)

- master在每次准备提交事务完成数据更新前，将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log event，简称event)
- slave启动一个I/O线程来读取主库上binary log中的事件，并记录到slave自己的中继日志(relay log)中。
- slave还会起动一个SQL线程，该线程从relay log中读取事件并在备库执行，从而实现备库数据的更新。



应用场景：

- 读写分离：最典型的场景就是通过Mysql主从之间通过binlog复制来实现横向扩展，来实现读写分离。有一个主库Master，所有的更新操作都在master上进行；同时会有多个Slave，每个Slave都连接到Master上，获取binlog在本地回放，实现数据复制。在应用层面，需要对执行的sql进行判断。所有的更新操作都通过Master(Insert、Update、Delete等)，而查询操作(Select等)都在Slave上进行。由于存在多个slave，所以我们可以在slave之间做负载均衡。通常业务都会借助一些数据库中间件，如tddl、sharding-jdbc等来完成读写分离功能。
- 数据恢复：一些同学可能有误删除数据库记录的经历，或者因为误操作导致数据库存在大量脏数据的情况。例如笔者，曾经因为误操作污染了业务方几十万数据记录。如何将脏数据恢复成原来的样子？如果恢复已经被删除的记录？这些都可以通过反解binlog来完成，笔者也是通过这个手段，来恢复业务方的记录。
- 数据最终一致性：在实际开发中，我们经常会遇到一些需求，在数据库操作成功后，需要进行一些其他操作，如：发送一条消息到MQ中、更新缓存或者更新搜索引擎中的索引等。如何保证数据库操作与这些行为的一致性，就成为一个难题。以数据库与redis缓存的一致性为例，我们通过一个组件，来模拟的mysql的slave，拉取并解析binlog中的信息。通过解析binlog的信息，去异步的更新缓存、索引或者发送MQ消息，保证数据库与其他组件中数据的最终一致。
- 可靠消息：可靠消息是指的是：保证本地事务与发送消息到MQ行为的一致性。一些业务使用本地事务表或者独立消息服务，来保证二者的最终一致。Apache RocketMQ在4.3版本开源了事务消息，也是用于完成此功能。事实上，这两种方案，都有一定侵入性，对业务不透明。通过订阅binlog来发送可靠消息，则是一种解耦、无侵入的方案。
- 缓存一致性：业务经常遇到的一个问题是，如何保证数据库中记录和缓存中数据的一致性。不妨换一种思路，只更新数据库，数据库更新成功后，通过拉取binlog来异步的更新缓存(通常是删除，让业务回源到数据库)。如果数据库更新失败，没有对应binlog，那么也不会去更新缓存，从而实现最终一致性。
- 异地多活：一个更大的应用场景，异地多活场景下，跨数据中心之间的数据同步。这种场景的下，多个数据中心都需要写入数据，并且往对方同步。这里有一些特殊的问题需要处理。典型的包括：数据冲突：双方同时插入了一个相同主键的值，那么往对方同步时，就会出现主键冲突的错误。数据回环：一个库A中插入的数据，通过binlog同步到另外一个库B中，依然会产生binlog。此时库B的数据再次同步回库A，如此反复，就形成了一个死循环。如何解决数据冲突、数据回环，就变成了binlog同步组件要解决的问题。同样，业界也有了成熟的实现，比较知名的有阿里开源的otter，以及摩拜(已经属于美团)的DRC等。
- 数据回环一个解决：INSERT导致的唯一性冲突：同步INSERT语句时违背了唯一性约束，例如双向同步的两个节点同时或者在极为接近的时间INSERT某一个主键值相同的记录，那么同步到对端时，会因为已经存在相同主键值的记录，导致Insert同步失败。 引用自： http://itindex.net/detail/61565-%E6%95%B0%E6%8D%AE-%E5%A4%8D%E5%88%B6-%E6%95%B0%E6%8D%AE

### 2. MVVC 原理

#### 2.1 MVCC出现背景是什么？

> 引用自：https://mp.weixin.qq.com/s/OYDfxgzNAOUGFILGk__CBQ

事务的4个隔离级别以及对应的3种异常：

![图片](images/WX20210823-192114@2-13x.png)

- 脏读：一个事务读取到了另外一个事务没有提交的数据；
- 不可重复读：在同一事务中，两次读取同一数据，得到内容不同；
- 幻读：同一事务中，用同样的操作读取两次，得到的记录数不相同。


在 MySQL中，默认的隔离级别是可重复读，可以解决脏读和不可重复读的问题，但不能解决幻读问题。如果我们想要解决幻读问题，就需要采用串行化的方式，也就是将隔离级别提升到最高，但这样一来就会大幅降低数据库的事务并发能力。而MVCC就是通过乐观锁的方式来解决不可重复读和幻读问题，它可以在大多数情况下替代行级锁，降低系统的开销。MySQL 并发事务会引起更新丢失问题，解决办法是锁，主要分两类：

- 乐观锁：其实现如同它的名字一样，是假设比较好的情况。每次取数据的时候都认为他人不会对其修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。
- 悲观锁：悲观锁也如同它的名字一样，总是假设比较坏的情况，每次取数据的时候都认为他人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。



> 引用自：https://liqitian.com/articles/2020/07/23/1595499457928.html

#### 2.2 MVCC解决了什么问题

MVCC是在并发访问数据库时，通过对数据做多版本管理，避免因为写锁的阻塞而造成读数据的并发阻塞问题。通俗的讲就是MVCC通过保存数据的历史版本，根据比较版本号来处理数据的是否显示，从而达到**读取数据的时候不需要加锁就可以保证事务隔离性的效果**。总结一下就是下边三个问题：

- 读写之间阻塞的问题，通过 MVCC 可以让读写互相不阻塞，即读不阻塞写，写不阻塞读，这样就可以提升事务并发处理能力。
- 降低了死锁的概率。这是因为 MVCC 采用了乐观锁的方式，读取数据时并不需要加锁，对于写操作，也只锁定必要的行。
- 解决一致性读的问题。一致性读也被称为快照读，当我们查询数据库在某个时间点的快照时，只能看到这个时间点之前事务提交更新的结果，而不能看到这个时间点之后事务提交的更新结果。

A正在读数据库中某些内容，而B正在给这些内容做修改（A,B为两个单独的事务），A可能看到一个不一致的数据，在B没有提交前，如何让A能够一直读到的数据都是一致的或读到的数据一直是最新的呢？

思路：一个支持MVCC的数据库，在更新某些数据时，并非使用新数据覆盖旧数据，而是标记旧数据是过时的，同时在其他地方新增一个数据版本。因此，同一份数据有多个版本存储，但只有一个是最新的。
MVCC提供了 时间一致性的 处理思路，在MVCC下读事务时，通常使用一个时间戳或者事务ID来确定访问哪个状态的数据库及哪些版本的数据。读事务跟写事务彼此是隔离开来的，彼此之间不会影响。假设同一份数据，既有读事务访问，又有写事务操作，实际上，写事务会新建一个新的数据版本，而读事务访问的是旧的数据版本，直到写事务提交，读事务才会访问到这个新的数据版本。每个用户连接数据库时，看到的都是某一特定时刻的数据库快照，在B的事务没有提交之前，A始终读到的是某一特定时刻的数据库快照，不会读到B事务中的数据修改情况，直到B事务提交，才会读取B的修改内容。

- MVCC使得数据库读不会对数据加锁，普通的SELECT请求不会加锁，提高了数据库的并发处理能力；
- 借助MVCC，数据库可以实现RC，RR等隔离级别，用户可以查看当前数据的前一个或者前几个历史版本。保证了ACID中的I特性（隔离性）。
- Read uncimmitted由于存在脏读，即能读到未提交事务的数据行，所以不适用MVCC.原因是MVCC的创建版本和删除版本只要在事务提交后才会产生。

另外两个概念

- 快照读：读取的是快照数据，不加锁的简单的SELECT都属于快照读（只是普通的读操作）。
- 当前读：当前读就是读取最新数据，而不是历史版本的数据。加锁的SELECT，或者对数据进行增删改都会进行当前读（包括加锁的读取和DML操作）。


#### 2.3. InnoDB的MVCC实现机制

每条记录都会保存两个隐藏列，事务id(trx_id)和回滚指针(roll_point)。每次操作都会生成一条undo log日志，回滚指针指向前一条记录。

##### 2.3.1 几个关键概念：

- 事务版本号：每次事务开启前都会从数据库获得一个自增长的事务ID，可以从事务ID判断事务的执行先后顺序。
- 表格的隐藏列：DB_TRX_ID: 记录操作该数据事务的事务ID；DB_ROLL_PTR：指向上一个版本数据在undo log 里的位置指针；DB_ROW_ID: 隐藏ID ，当创建表没有合适的索引作为聚集索引时，会用该隐藏ID创建聚集索引;
- Undo log：Undo log 主要用于记录数据被修改之前的日志，在表信息修改之前先会把数据拷贝到undo log 里，当事务进行回滚时可以通过undo log 里的日志进行数据还原。
- Read view：在innodb中每个SQL语句执行前都会得到一个read_view。副本主要保存了当前数据库系统中正处于活跃（没有commit）的事务的ID号，其实简单的说这个副本中保存的是系统中当前不应该被本事务看到的其他事务id列表。Read view 的几个重要属性


##### 2.3.2 版本链

`roll_pointer`每次对记录修改的时候，都会把老版本写入undo log中。新纪录中`roll_pointer`就是存了一个指针，它指向这条记录的上一个版本的位置，通过它来获得上一个版本的记录信息。记录每次更新后，都会将旧值放到一条undo日志中，就算是该记录的一个旧版本，随着更新次数的增多，所有的版本都会被roll_pointer属性连接成一个链表，我们把这个链表称之为版本链，版本链的头节点就是当前记录最新的值。另外，每个版本中还包含生成该版本时对应的事务id，这个信息很重要，我们稍后就会用到。

![图片](images/WX20210823-192114@2-12x.png)

##### 2.3.3 ReadView

对于使用READ UNCOMMITTED隔离级别的事务来说，直接读取记录的最新版本就好了，对于使用SERIALIZABLE隔离级别的事务来说，使用加锁的方式来访问记录。对于使用READ COMMITTED和REPEATABLE READ隔离级别的事务来说，就需要用到我们上边所说的版本链了，核心问题就是：**需要判断一下版本链中的哪个版本是当前事务可见的。所以后面提出了一个ReadView的概念，这个ReadView中主要包含当前系统中还有哪些活跃的（未提交的）读写事务，把它们的事务id放到一个列表中，我们把这个列表命名为为m_ids和表示生成该ReadView的快照读操作产生的事务id(creator_trx_id)**。这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见：

- 如果被访问版本的trx_id属性值与ReadView中的creator_trx_id值相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。
- 如果被访问版本的trx_id属性值小于m_ids列表中最小的事务id，表明生成该版本的事务在生成ReadView前已经提交，所以该版本可以被当前事务访问。
- 如果被访问版本的trx_id属性值大于m_ids列表中最大的事务id，表明生成该版本的事务在生成ReadView后才生成，所以该版本不可以被当前事务访问。
- 如果被访问版本的trx_id属性值在m_ids列表中最大的事务id和最小事务id之间，那就需要判断一下trx_id属性值是不是在m_ids列表中，如果在，说明创建ReadView时生成该版本的事务还是活跃的，该版本不可以被访问；如果不在，说明创建ReadView时生成该版本的事务已经被提交，该版本可以被访问。
- 如果某个版本的数据对当前事务不可见的话，那就顺着版本链找到下一个版本的数据，继续按照上边的步骤判断可见性，依此类推，直到版本链中的最后一个版本，如果最后一个版本也不可见的话，那么就意味着该条记录对该事务不可见，查询结果就不包含该记录。

更详细的解释：

- 数据事务ID <up_limit_id 则显示：如果数据事务ID小于read view中的最小活跃事务ID，则可以肯定该数据是在当前事务启之前就已经存在了的,所以可以显示。
- 数据事务ID>=low_limit_id 则不显示：如果数据事务ID大于read view 中的当前系统的最大事务ID，则说明该数据是在当前read view 创建之后才产生的，所以数据不予显示。
- up_limit_id <=数据事务ID<low_limit_id 则与活跃事务集合trx_ids里匹配，如果数据的事务ID大于最小的活跃事务ID,同时又小于等于系统最大的事务ID，这种情况就说明这个数据有可能是在当前事务开始的时候还没有提交的。所以这时候我们需要把数据的事务ID与当前read view 中的活跃事务集合trx_ids 匹配:

    - 情况1: 如果事务ID不存在于trx_ids 集合（则说明read view产生的时候事务已经commit了），这种情况数据则可以显示。
    - 情况2： 如果事务ID存在trx_ids则说明read view产生的时候数据还没有提交，但是如果数据的事务ID等于creator_trx_id ，那么说明这个数据就是当前事务自己生成的，自己生成的数据自己当然能看见，所以这种情况下此数据也是可以显示的。
    - 情况3： 如果事务ID既存在trx_ids而且又不等于creator_trx_id那就说明read view产生的时候数据还没有提交，又不是自己生成的，所以这种情况下此数据不能显示。

- 不满足read view条件时候，从undo log里面获取数据：当数据的事务ID不满足read view条件时候，从undo log里面获取数据的历史版本，然后数据历史版本事务号回头再来和read view 条件匹配 ，直到找到一条满足条件的历史数据，或者找不到则返回空结果；

> 这里边有非常好的例子：https://zhuanlan.zhihu.com/p/52977862

> 非常棒的例子：https://segmentfault.com/a/1190000039809030，https://blog.51cto.com/u_12182612/2486731，https://www.modb.pro/db/74300，https://blog.csdn.net/qq_35190492/article/details/109044141

> 在MySQL中，READ COMMITTED和REPEATABLE READ隔离级别的的一个非常大的区别就是它们生成ReadView的时机不同。

##### 2.3.4 MVCC不存在幻读问题

首先确认一点MVCC属于快照读的，在进行快照读的情况下是不会对数据进行加锁，而是基于事务版本号和undo历史版本读取数据，其实上面的文章已经说得很清楚了，我们根据上面的MVCC流程来推导，无论如何在MVCC的情况下都是不会出现幻读的问题的，如下图。

- 开启事务1，获得事务ID为1。
- 事务1执行查询，得到readview。
- 开始事务2。
- 执行insert。
- 提交事务2。
- 执行事务1的第二次查询 (因为这里是RR级别，所以不会再去获得readview,还是使用第一次获得的readview)
- 最后得到的结果是，插入的数据不会显示，因为插入的数据事务ID大于等于 readview里的最大活跃事务ID。

##### 2.3.5 RR级别下的例子

```
# Transaction 100

BEGIN;
UPDATE t SET c = '关羽' WHERE id = 1;
UPDATE t SET c = '张飞' WHERE id = 1;
```

```
# Transaction 200
BEGIN;

# 更新了一些别的表的记录
```

假设现在有一个使用REPEATABLE READ隔离级别的事务开始执行：

```
# 使用REPEATABLE READ隔离级别的事务
BEGIN;

# SELECT1：Transaction 100、200未提交
SELECT * FROM t WHERE id = 1; # 得到的列c的值为'刘备'
```

这个SELECT1的执行过程如下：

在执行SELECT语句时会先生成一个ReadView，ReadView的m_ids列表的内容就是[100, 200]。然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列c的内容是'张飞'，该版本的trx_id值为100，在m_ids列表内，所以不符合可见性要求，根据roll_pointer跳到下一个版本。下一个版本的列c的内容是'关羽'，该版本的trx_id值也为100，也在m_ids列表内，所以也不符合要求，继续跳到下一个版本。下一个版本的列c的内容是'刘备'，该版本的trx_id值为80，小于m_ids列表中最小的事务id100，所以这个版本是符合要求的，最后返回给用户的版本就是这条列c为'刘备'的记录。

之后，我们把事务id为100的事务提交一下，就像这样：

```
# Transaction 100
BEGIN;
UPDATE t SET c = '关羽' WHERE id = 1;
UPDATE t SET c = '张飞' WHERE id = 1;
COMMIT;
```

然后再到事务id为200的事务中更新一下表t中id为1的记录：

```
# Transaction 200
BEGIN;
# 更新了一些别的表的记录
...
UPDATE t SET c = '赵云' WHERE id = 1;
UPDATE t SET c = '诸葛亮' WHERE id = 1;
```

然后再到刚才使用REPEATABLE READ隔离级别的事务中继续查找这个id为1的记录，如下：

```
# 使用REPEATABLE READ隔离级别的事务
BEGIN;

# SELECT1：Transaction 100、200均未提交
SELECT * FROM t WHERE id = 1; # 得到的列c的值为'刘备'

# SELECT2：Transaction 100提交，Transaction 200未提交
SELECT * FROM t WHERE id = 1; # 得到的列c的值仍为'刘备'
```
这个SELECT2的执行过程如下：

因为之前已经生成过ReadView了，所以此时直接复用之前的ReadView，之前的ReadView中的m_ids列表就是[100, 200]。
然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列c的内容是'诸葛亮'，该版本的trx_id值为200，在m_ids列表内，所以不符合可见性要求，根据roll_pointer跳到下一个版本。
下一个版本的列c的内容是'赵云'，该版本的trx_id值为200，也在m_ids列表内，所以也不符合要求，继续跳到下一个版本。
下一个版本的列c的内容是'张飞'，该版本的trx_id值为100，而m_ids列表中是包含值为100的事务id的，所以该版本也不符合要求，同理下一个列c的内容是'关羽'的版本也不符合要求。继续跳到下一个版本。
下一个版本的列c的内容是'刘备'，该版本的trx_id值为80，80小于m_ids列表中最小的事务id100，所以这个版本是符合要求的，最后返回给用户的版本就是这条列c为'刘备'的记录。

#### 2.4 MVCC总结

MVCC 的核心就是 Undo Log+ Read View。

- “MV”就是通过 Undo Log 来保存数据的历史版本，实现多版本的管理；
- “CC”是通过 Read View 来实现管理，通过 Read View 原则来决定数据是否显示。

MVCC（Multi-Version Concurrency Control ，多版本并发控制）指的就是在使用`READ COMMITTD`、`REPEATABLE READ`这两种隔离级别的事务在执行普通的`SEELCT`操作时访问记录的版本链的过程，这样子可以使不同事务的`读-写`、`写-读`操作并发执行，从而提升系统性能。`READ COMMITTD`、`REPEATABLE READ`这两个隔离级别的一个很大不同就是生成`ReadView`的时机不同，`READ COMMITTD`在每一次进行普通`SELECT`操作前都会生成一个`ReadView`，而`REPEATABLE READ`只在第一次进行普通`SELECT`操作前生成一个`ReadView`，之后的查询操作都重复这个`ReadView`就好了。

所谓的MVCC（Multi-Version Concurrency Control ，多版本并发控制）指的就是在使用 READ COMMITTD 、REPEATABLE READ 这两种隔离级别的事务在执行普通的 SEELCT 操作时访问记录的版本链的过程，这样子可以使不同事务的 读-写 、 写-读 操作并发执行，从而提升系统性能。

在 MySQL 中， READ COMMITTED 和 REPEATABLE READ 隔离级别的的一个非常大的区别就是它们生成 ReadView 的时机不同。在 READ COMMITTED 中每次查询都会生成一个实时的 ReadView，做到保证每次提交后的数据是处于当前的可见状态。而 REPEATABLE READ 中，在当前事务第一次查询时生成当前的 ReadView，并且当前的 ReadView 会一直沿用到当前事务提交，以此来保证可重复读（REPEATABLE READ）。

** RC是读已提交，是因为MVVC每次读取的时候是不同的ReadView要重新判断，RR是可重复读，使用的是第一次读的ReadView所以是可以重复读的。**


### 3. 锁的实现原理

> 引用自：https://mp.weixin.qq.com/s/D50ledyMfn84u06RNiQSmw

#### 3.1 锁的分类

![图片](images/WX20210823-192114@2-14x.png)

##### 3.1.1 按使用方式

- 乐观锁：机制采取了更加宽松的加锁机制。悲观锁大多数情况下依靠数据库的锁机制实现，以保证操作最大程度的独占性。但随之而来的就是数据库性能的大量开销，特别是对长事务而言，这样的开销往往无法承受。相对悲观锁而言，乐观锁更倾向于开发运用。
- 悲观锁：具有强烈的独占和排他特性。它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。

##### 3.1.2 按粒度

- 表级锁 是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MyISAM与InnoDB都支持表级锁定。表级锁分为表共享读锁与表独占写锁。
- 行级锁 是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。
- 页级锁 是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。BDB支持页级锁。

这里需要说明的是，乐观锁与悲观锁并不是数据库中实现的锁机制，是需要我们自己去实现的。它是一种思想，我们不仅仅可以用在MySQL中，其它地方有需要的也可以用到。而像悲观锁它也是利用数据库现有的机制进行实现的。下面先根据不同分类对说明相关概念。

> 引用自：https://juejin.cn/post/6855129007336521741

##### 3.1.3 Mysql锁分类

- 共享锁(Shared Locks)：简称S锁，在事务要读取一条记录时，需要先获取该记录的S锁。S锁可以在同一时刻被多个事务同时持有。我们可以用select ...... lock in share mode;的方式手工加上一把S锁。
- 排他锁(Exclusive Locks)：简称X锁，在事务要改动一条记录时，需要先获取该记录的X锁。X锁在同一时刻最多只能被一个事务持有。X锁的加锁方式有两种，第一种是自动加锁，在对数据进行增删改的时候，都会默认加上一个X锁。还有一种是手工加锁，我们用一个FOR UPDATE给一行数据加上一个X锁。
- 意向锁(Intention Locks)：除了共享锁(Shared Locks)和排他锁(Exclusive Locks)，Mysql还有意向锁(Intention Locks)。意向锁是由数据库自己维护的，一般来说，当我们给一行数据加上共享锁之前，数据库会自动在这张表上面加一个意向共享锁(IS锁)；当我们给一行数据加上排他锁之前，数据库会自动在这张表上面加一个意向排他锁(IX锁)。**意向锁可以认为是S锁和X锁在数据表上的标识，通过意向锁可以快速判断表中是否有记录被上锁，从而避免通过遍历的方式来查看表中有没有记录被上锁，提升加锁效率。**例如，我们要加表级别的X锁，这时候数据表里面如果存在行级别的X锁或者S锁的，加锁就会失败，此时直接根据意向锁就能知道这张表是否有行级别的X锁或者S锁。InnoDB的意向锁主要用户多粒度的锁并存的情况。比如事务A要在一个表上加S锁，如果表中的一行已被事务B加了X锁，那么该锁的申请也应被阻塞。如果表中的数据很多，逐行检查锁标志的开销将很大，系统的性能将会受到影响。为了解决这个问题，可以在表级上引入新的锁类型来表示其所属行的加锁情况，这就引出了“意向锁”的概念。举个例子，如果表中记录1亿，事务A把其中有几条记录上了行锁了，这时事务B需要给这个表加表级锁，如果没有意向锁的话，那就要去表中查找这一亿条记录是否上锁了。如果存在意向锁，那么假如事务Ａ在更新一条记录之前，先加意向锁，再加Ｘ锁，事务B先检查该表上是否存在意向锁，存在的意向锁是否与自己准备加的锁冲突，如果有冲突，则等待直到事务Ａ释放，而无须逐条记录去检测。事务Ｂ更新表时，其实无须知道到底哪一行被锁了，它只要知道反正有一行被锁了就行了。

> 引用详情：https://juejin.cn/post/6855129007336521741

#### 3.2 Mysql锁详情

##### 3.2.1 InnoDB中的表级锁

InnoDB中的表级锁主要包括表级别的意向共享锁(IS锁)和意向排他锁(IX锁)以及自增锁(AUTO-INC锁)。其中IS锁和IX锁在前面已经介绍过了，这里不再赘述，我们接下来重点了解一下AUTO-INC锁。
大家都知道，如果我们给某列字段加了AUTO_INCREMENT自增属性，插入的时候不需要为该字段指定值，系统会自动保证递增。系统实现这种自动给AUTO_INCREMENT修饰的列递增赋值的原理主要是两个：

- AUTO-INC锁：在执行插入语句的时先加上表级别的AUTO-INC锁，插入执行完成后立即释放锁。如果我们的插入语句在执行前无法确定具体要插入多少条记录，比如INSERT ... SELECT这种插入语句，一般采用AUTO-INC锁的方式。
- 轻量级锁：在插入语句生成AUTO_INCREMENT值时先才获取这个轻量级锁，然后在AUTO_INCREMENT值生成之后就释放轻量级锁。如果我们的插入语句在执行前就可以确定具体要插入多少条记录，那么一般采用轻量级锁的方式对AUTO_INCREMENT修饰的列进行赋值。这种方式可以避免锁定表，可以提升插入性能。

##### 3.2.2 InnoDB中的行级锁

前面说过，通过**MVCC可以解决脏读、不可重复读、幻读这些读一致性问题**，但实际上这只是解决了普通select语句的数据读取问题。事务利用MVCC进行的读取操作称之为快照读，所有普通的SELECT语句在READ COMMITTED、REPEATABLE READ隔离级别下都算是快照读。除了快照读之外，还有一种是锁定读，即在读取的时候给记录加锁，**在锁定读的情况下依然要解决脏读、不可重复读、幻读的问题**。由于都是在记录上加锁，这些锁都属于行级锁。
InnoDB的行锁，是通过锁住索引来实现的，如果加锁查询的时候没有使用过索引，会将整个聚簇索引都锁住，相当于锁表了。根据锁定范围的不同，行锁可以使用记录锁(Record Locks)、间隙锁(Gap Locks)和临键锁(Next-Key Locks)的方式实现。假设现在有一张表t，主键是id。我们插入了4行数据，主键值分别是 1、4、7、10。接下来我们就以聚簇索引为例，具体介绍三种形式的行锁。

- 记录锁(Record Locks)：所谓记录，就是指聚簇索引中真实存放的数据，比如上面的1、4、7、10都是记录。当我们使用唯一性的索引(包括唯一索引和聚簇索引)进行等值查询且精准匹配到一条记录时，此时就会直接将这条记录锁定。例如select * from t where id =4 for update;就会将id=4的记录锁定。
- 间隙锁(Gap Locks)： 间隙指的是两个记录之间逻辑上尚未填入数据的部分，比如上述的(1,4)、(4,7)等。当我们使用用等值查询或者范围查询，并且没有命中任何一个record，此时就会将对应的间隙区间锁定。例如select * from t where id =3 for update;或者select * from t where id > 1 and id < 4 for update;就会将(1,4)区间锁定。
- 临键锁(Next-Key Locks)： 临键指的是间隙加上它右边的记录组成的左开右闭区间。比如上述的(1,4]、(4,7]等。临键锁就是记录锁(Record Locks)和间隙锁(Gap Locks)的结合，即除了锁住记录本身，还要再锁住索引之间的间隙。当我们使用范围查询，并且命中了部分record记录，此时锁住的就是临键区间。注意，临键锁锁住的区间会包含最后一个record的右边的临键区间。例如select * from t where id > 5 and id <= 7 for update;会锁住(4,7]、(7,+∞)。mysql默认行锁类型就是临键锁(Next-Key Locks)。当使用唯一性索引，等值查询匹配到一条记录的时候，临键锁(Next-Key Locks)会退化成记录锁；没有匹配到任何记录的时候，退化成间隙锁。
- 插入意向锁：Insert Intention Locks，插入意向锁，是一种特殊的间隙锁，只有在执行INSERT操作时才会加锁，插入意向锁之间不冲突，可以向一个间隙中同时插入多行数据，但插入意向锁与间隙锁是冲突的，当有间隙锁存在时，插入语句将被阻塞，正是这个特性解决了幻读的问题。假设有一个记录索引包含键值4和7，不同的事务分别插入5和6，每个事务都会产生一个加在4-7之间的插入意向锁，获取在插入行上的排它锁，但是不会被互相锁住，因为数据行并不冲突。

**间隙锁(Gap Locks)和临键锁(Next-Key Locks)都是用来解决幻读问题的，**在已提交读（READ COMMITTED）隔离级别下，间隙锁(Gap Locks)和临键锁(Next-Key Locks)都会失效！

> 引用自：https://mp.weixin.qq.com/s/T9bZCKHlFLEkH-lvAOSo7g

##### 3.2.3 何时加锁

- SELECT xxx 查询语句正常情况下为快照读，不加锁；
- SELECT xxx LOCK IN SHARE MODE 语句为当前读，加 S 锁；
- SELECT xxx FOR UPDATE 语句为当前读，加 X 锁；
- DML 语句（INSERT、DELETE、UPDATE）为当前读，加 X 锁；
- DDL 语句（ALTER、CREATE 等）加表级锁，且是隐式提交不能回滚；


> 引用自：https://mp.weixin.qq.com/s/QVEUIfD0RBbtvUDORaz2vQ

#### 3.3 锁原理

##### 3.3.1 表锁原理

表锁由MySQL Server实现，一般在执行DDL语句时会对整个表进行加锁，比如说ALTER TABLE等操作。在执行 SQL 语句时，也可以明确指定对某个表进行加锁。表锁使用的是一次性锁技术，也就是说，在会话开始的地方使用 lock 命令将后续需要用到的表都加上锁，在表释放前，只能访问这些加锁的表，不能访问其他表，直到最后通过 unlock tables 释放所有表锁。除了使用 unlock tables 显示释放锁之外，会话持有其他表锁时执行lock table 语句会释放会话之前持有的锁；会话持有其他表锁时执行 start transaction 或者 begin 开启事务时，也会释放之前持有的锁。

##### 3.3.2 行锁原理


不同存储引擎的行锁实现不同，后续没有特别说明，则行锁特指 InnoDB 实现的行锁。

在了解 InnoDB 的加锁原理前，需要对其存储结构有一定的了解。InnoDB是聚簇索引，也就是B+树的叶节点既存储了主键索引也存储了数据行。而InnoDB的二级索引的叶节点存储的则是主键值，所以通过二级索引查询数据时，还需要拿对应的主键去聚簇索引中再次进行查询。关于 InnoDB 和 MyISAM 的索引的详细知识可以阅读《Mysql探索(一):B+Tree索引》一文。

![图片](images/WX20210823-192114@2-15x.png)

下面以两条 SQL 的执行为例，讲解一下 InnoDB 对于单行数据的加锁原理。

```
update user set age = 10 where id = 49 ; 
update user set age = 10 where name = 'Tom' ;
```

第一条 SQL使用主键索引来查询，则只需要在id=49这个主键索引上加上写锁；第二条SQL则使用二级索引来查询，则首先在name=Tom这个索引上加写锁，然后由于使用InnoDB二级索引还需再次根据主键索引查询，所以还需要在 id=49这个主键索引上加写锁，如上图所示。也就是说使用主键索引需要加一把锁，使用二级索引需要在二级索引和主键索引上各加一把锁。根据索引对单行数据进行更新的加锁原理了解了，那如果更新操作涉及多个行呢，比如下面 SQL 的执行场景。
```
update user set age = 10 where id > 49 ;
```
MySQL Server会根据WHERE条件读取第一条满足条件的记录，然后InnoDB引擎会将第一条记录返回并加锁，接着MySQLServer发起更新改行记录的UPDATE请求，更新这条记录。一条记录操作完成，再读取下一条记录，直至没有匹配的记录为止。

在日常操作中，UPDATE、INSERT、DELETE InnoDB会自动给涉及的数据集加排他锁，一般的 SELECT 一般是不加任何锁的。我们可以使用以下方式显示的为 SELECT 加锁。
```
共享锁：select * from table_name where id =10 lock in share mode;
排他锁：select * from table_name where id=10 for update;
```

#### 3.4 其他

#### 3.4.1 查看锁的情况

mysql> show status like 'innodb_row_lock%';
+-------------------------------+--------+
| Variable_name                 | Value  |
+-------------------------------+--------+
| Innodb_row_lock_current_waits | 0      |
| Innodb_row_lock_time          | 218276 |
| Innodb_row_lock_time_avg      | 18189  |
| Innodb_row_lock_time_max      | 51058  |
| Innodb_row_lock_waits         | 12     |
+-------------------------------+--------+
5 rows in set (0.05 sec)

#### 3.4.2 InnoDB什么时候会锁表

- InnoDB这种行锁实现特点意味者：只有通过索引条件检索数据，InnoDB才会使用行级锁，否则，InnoDB将使用表锁！
- ALTER TABLE之类的语句会使用表锁，忽略存储引擎的锁机制。表锁的语法是 lock tables … read/write。
- insert时全表锁，因为要生成主键字段、索引等等，update是行级锁。

这片文章根据以下各种情况分析了加锁的过程：https://weikeqin.com/2019/09/05/mysql-lock-table-solution/

```
update t1 set update_time = now() where k = 10 ;
```
InnoDB的加锁分析前提条件
前提一:查询列是不是主键？
前提二:当前系统的隔离级别是什么？
前提三:查询列上有索引吗？
前提四:查询列是唯一索引吗？
前提五:两个SQL的执行计划是什么？索引扫描？全表扫描？

组合：

组合一：k列是主键，RC隔离级别：Read Committed 隔离级别，k列是主键，给定SQL：update t1 set update_time = now() where k = 10; 只需要将主键上 k = 10的记录加上X锁即可
组合二：k列是二级唯一索引，RC隔离级别：Read Committed 隔离级别，k列有unique索引，unique索引上的k=10一条记录加上X锁，同时，会根据读取到的列，回主键索引(聚簇索引)，然后将聚簇索引上对应的主键索引项加X锁。
组合三：k列是二级非唯一索引，RC隔离级别：ReadCommitted隔离级别，k列上有索引，那么对应的所有满足SQL查询条件的记录，都会被加锁。同时，这些记录在主键索引上的记录，然后将聚簇索引上对应的主键索引项加X锁。
组合四：k列上没有索引，RC隔离级别：Read Committed 隔离级别，若k列上没有索引，SQL会走聚簇索引的全扫描进行过滤，由于过滤是由MySQL Server层面进行的。因此每条记录，无论是否满足条件，都会被加上X锁。但是，为了效率考量，MySQL做了优化，对于不满足条件的记录，会在判断后放锁，最终持有的，是满足条件的记录上的锁，但是不满足条件的记录上的加锁/放锁动作不会省略。同时，优化也违背了2PL的约束。
组合五： k列是主键，RR隔离级别：Repeatable Read 隔离级别，k列是主键列，给定SQL update t1 set update_time = now() where k = 10; 只需要将主键上 k = 10的记录加上X锁即可。
组合六： k列是二级唯一索引，RR隔离级别：Repeatable Read 隔离级别，k列有unique索引，unique索引上的k=10一条记录加上X锁，同时，会根据读取到的列，回主键索引(聚簇索引)，然后将聚簇索引上对应的主键索引项加X锁。
组合七： k列是二级非唯一索引，RR隔离级别：Repeatable Read 隔离级别，k列有索引， 通过索引定位到第一条满足查询条件的记录，加记录上的X锁，加GAP上的GAP锁，然后加主键聚簇索引上的记录X锁，然后返回；然后读取下一条，重复进行。直至进行到第一条不满足条件的记录，此时，不需要加记录X锁，但是仍旧需要加GAP锁，最后返回结束。考虑到B+树索引的有序性，满足条件的项一定是连续存放的。如果要插入一条记录，肯定会插入在相同位置，为了保证两次查询查到的值一致，MySQL选择了用GAP锁，将 查询值范围前、查询值范围、查询值范围后 三个GAP给锁起来。GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。
组合八： k列上没有索引，RR隔离级别：在Repeatable Read隔离级别下，如果进行全表扫描的当前读，那么会锁上表中的所有记录，同时会锁上聚簇索引内的所有GAP，杜绝所有的并发 更新/删除/插入 聚簇索引上的所有记录，都被加上了X锁。其次，聚簇索引每条记录间的间隙(GAP)，也同时被加上了GAP锁。
组合九： Serializable隔离级别：Serializable隔离级别下直接用加锁的方式来避免并行访问。在RC，RR隔离级别下，都是快照读，不加锁。Serializable隔离级别，读不加锁就不再成立，所有的读操作，都是当前读。

#### 3.4.3 死锁检测

以事务为顶点，以事务请求的锁为边，构建一个有向图，这个图被称为Wait-for Graph。比如事务A要请求锁1、锁2，而锁1、锁2分别被事务B、事务C持有，因此事务A依赖事务B、事务C；事务B要请求锁3，而锁3被事务C持有，所以事务B依赖事务C；事务C要请求锁4，而锁4被事务A持有，所以事务C依赖事务A；依此类推。死锁检测就是发现这种有向图中存在的环，本图中就是事务A、事务B、事务C之间出现了环，所以发生了死锁。关于如何判断一个有向图是否存在环属于图论中的基本问题，存在多种算法，此处不展开讨论。检测到死锁后，数据库可以强制让其中某个事务回滚，释放掉锁，把环断开，死锁就解除了。

> 引用自：https://mp.weixin.qq.com/s/JHRrt_3tgE-JdAxWH5vNNw

INFORMATION_SCHEMA提供对数据库元数据的访问、关于MySQL服务器的信息，如数据库或表的名称、列的数据类型或访问权限。其中有一个关于InnoDB数据库引擎表的集合，里面有记录数据库事务和锁的相关表。MySQL有关事务和锁的四条命令：

- SELECT * FROM information_schema.INNODB_TRX;命令是用来查看当前运行的所有事务。
- SELECT * FROM information_schema.INNODB_LOCKs;命令是用来查看当前出现的锁。
- SELECT * FROM information_schema.INNODB_LOCK_waits;命令是用来查看锁等待的对应关系。

show engine innodb status \G;命令是用来获取最近一次的死锁信息。在查询结果中可以看到是否有表锁等待或者死锁。如果有死锁发生，可以通过KILL trx_mysql_thread_id来杀掉当前运行的事务。

> 引用自：https://mp.weixin.qq.com/s/yzXbbutzVJ1hIZgVszIBgw

#### 3.4.4 如何尽可能避免死锁：

- 合理的设计索引，区分度高的列放到组合索引前面，使业务SQL尽可能通过索引定位更少的行，减少锁竞争。
- 尽量按主键/索引去查找记录，范围查找增加了锁冲突的可能性，也不要利用数据库做一些额外额度计算工作。比如有的程序会用到select … where … order by rand();这样的语句，类似这样的语句用不到索引，因此将导致整个表的数据都被锁住。
- 大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。
- 以固定的顺序访问表和行。比如两个更新数据的事务，事务A更新数据的顺序为1，2;事务B更新数据的顺序为2，1。这样更可能会造成死锁。
- 降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。
- 以固定的顺序访问表和行。交叉访问更容易造成事务等待回路。
- 降低隔离级别。如果业务允许(上面4.3也分析了，某些业务并不能允许)，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。
- 为表添加合理的索引。防止没有索引出现表锁，出现的死锁的概率会突增。

携程文档：https://mp.weixin.qq.com/s/zi86vvktNbA0_R6s1P67iQ

> 引用自：https://mp.weixin.qq.com/s/Wc6Gw6S5xMy2DhTCrogxVQ

#### 3.4.5 为什么不建议使用订单号作为主键?

数据是以行为单位存储在聚簇索引里的，根据主键查询可以直接利用聚簇索引定位到所在记录，根据普通索引查询需要先在普通索引上找到对应的主键的值，然后根据主键值去聚簇索引上查找记录，俗称回表。

普通索引上存储的值是主键的值，如果主键是一个很长的字符串并且建了很多普通索引，将造成普通索引占有很大的物理空间，这也是为什么建议使用 自增ID 来替代订单号作为主键，另一个原因是 自增ID 在插入的时候可以保证相邻的两条记录可能在同一个数据块，而订单号的连续性在设计上可能没有自增ID好，导致连续插入可能在多个数据块，增加了磁盘读写次数。

如果我们查询一整行记录的话，一定要去聚簇索引上查找，而如果我们只需要根据普通索引查询主键的值，由于这些值在普通索引上已经存在，所以并不需要回表，这个称为索引覆盖，在一定程度上可以提高查询效率，由于联合索引上通过多个列构建索引，有时候我们可以将需要频繁查询的字段加到联合索引里面，例如如果经常需要根据 name 查找 age 我们可以建一个 name 和 age 的联合索引。

查询的时候如果在索引上用了函数，将导致无法用到根据之前列上的值构建的索引，索引遵循最左匹配原则，所以如果需要查询某个列的值中间是否包含某个字符串，将无法利用索引，如果有这种需求可以利用全文索引，而如果查询是否以某个字符串开头就可以，联合索引根据第一个列查询可以用到索引，仅仅根据第二个列将无法用到索引，查询的时候用 IN 的效率高于 NOT = 。另外建议将索引的列设置为非空，这个和 NULL 字段的存储有关，下文在分析。


> 引用自：https://segmentfault.com/a/1190000025156465

> 引用自：https://blog.csdn.net/jyxmust/article/details/77835848

### 4. 隔离级别的实现原理

#### 4.1 事务并发导致的问题

通俗地讲，事务就是一个“代码块”，这个代码块要么不执行，要么全部执行，事务并发导致的几类问题：

- 脏读：事务A读取了一条记录的值，然后基于这个值做业务逻辑，在事务A提交之前，事务B回滚了该记录，导致事务A读到的这条记录一个脏数据
- 不可重复读：在同一个事务里面，两次读取同一行记录，但结果不一样。因为另外一个事务在对此记录进行update操作
- 幻读：在同一个事务里面，同样的select语句，执行两次，返回的记录条数不一样。因为另外一个事务在进行insert/delete操作
- 丢失更新：两个事务同时修改同一条记录，事务A的修改被事务B覆盖了。举个例子：x=5，A和B同时把x读出来，减1，再写回去，得到x=4，实际x的正确值应该是x=3

#### 4.2 事务并发解决方案（数据库隔离级别）

为了解决上面几类问题，数据库设置了不同的事务隔离级别。不同数据库在事务隔离级别的定义和实现上会有差异，下面以MySQL InnoDB引擎为例，分析隔离级别是如何定义的，

- RU（Read Uncommited）：相当于什么都没做，上面的四个问题一个都没有解决，所以实际中不会采用
- RC（Read Commited）：只解决了上面的问题（1）脏读
- RR（Repeatable Read）：解决了上面的问题（1）（2）（3）这也是InnoDB默认的隔离级别
- Serialization：串行化。完全解决上面的四个问题
  隔离级别，一级比一级严格。隔离级别4就是串行化，所有事务串行执行，虽然能解决上面的四个问题，但性能无法接受，所以一般不会采用；隔离级别1没有任何作用，也不会采用；所以常用的是隔离级别2和隔离级别3。
  既然默认的隔离级别是3（RR），如何解决最后一个问题，丢失更新呢？这涉及到了悲观锁和乐观锁。

- 利用单条语句的原子性：在上面的每个事务里，都是把数据先select出来，再update回去，没有办法保证两条语句的原子性。如果改成一条语句，就能保证原子性。这种方法简单可行，但很有局限性。因为实际的业务场景往往需要把balance先读出来，做各种逻辑计算之后再写回去。如果不读，直接修改balance，没有办法知道修改之前的balance的值是多少。
- 悲观锁：悲观锁，就是认为数据发生并发冲突的概率很大，所以读之前就上锁。利用select xxx for update语句。悲观锁有潜在问题，假如事务A在拿到锁之后、Commit之前出问题了，会造成锁不能释放，数据库死锁。另外，一个事务拿到锁之后，其他访问该记录的事务都会被阻塞，这在高并发场景下会造成用户端的大量请求阻塞。
- 乐观锁：对于乐视锁，认为数据发生并发冲突的概率比较小，所以读之前不上锁。等到写回去的时候再判断数据是否被其他事务改了，即多线程里面经常会讲的CAS（Comapre AndSet）的思路。**CAS的核心思想是**：数据读出来的时候有一个版本v1，然后在内存里面修改，当再写回去的时候，如果发现数据库中的版本不是v1（比v1大），说明在修改的期间内别的事务也在修改，则放弃更新，把数据重新读出来，重新计算逻辑，再重新写回去，如此不断地重试。**在实现层面**，就是利用update语句的原子性实现了CAS，当且仅当version=v1时，才能把balance更新成功。在更新balance的同时，version也必须加1。version的比较、version的加1、balance的更新，这三件事情都是在一条update语句里面完成的，这是这个事情的关键所在！
- 分布式锁：乐观锁的方案可以很好地应对上述场景

所以现在的并发方案都是RR + 自己代码实现，保证效率的同时也保障安全性。


#### 4.3 事务原理（基于锁的方式，正确的是基于MVCC的）

> 引用自：https://mp.weixin.qq.com/s/glcsuhr2cc7S7G2z-TAmyA

相对于传统隔离级别基于锁的实现方式，MySQL是通过MVCC（多版本并发控制）来实现读-写并发控制，又是通过两阶段锁来实现写-写并发控制的。MVCC是一种无锁方案，用以解决事务读-写并发的问题，能够极大提升读-写并发操作的性能。**好好看MVCC的实现原理**

READ_UNCOMMITED 的原理:事务对当前**被读取的数据不加锁**；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加**行级共享锁，直到事务结束才释放**。
表现：事务1读取某行记录时，事务2也能对这行记录进行读取、更新；当事务2对该记录进行更新时，事务1再次读取该记录，能读到事务2对该记录的修改版本，即使该修改尚未被提交。事务1更新某行记录时，事务2不能对这行记录做更新，直到事务1结束。

READ_COMMITED 的原理:事务对当前被读取的数据加**行级共享锁（当读到时才加锁），一旦读完该行，立即释放该行级共享锁**；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其**加行级排他锁，直到事务结束才释放**。
表现：事务1读取某行记录时，事务2也能对这行记录进行读取、更新；当事务2对该记录进行更新时，事务1再次读取该记录，读到的只能是事务2对其更新前的版本，要不就是事务2提交后的版本；事务1更新某行记录时，事务2不能对这行记录做更新，直到事务1结束。

REPEATABLE READ的原理:事务在读取某数据的瞬间（就是开始读取的瞬间），必须先对其加**行级共享锁，直到事务结束才释放**；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其**加行级排他锁，直到事务结束才释放**。
表现：事务1读取某行记录时，事务2也能对这行记录进行读取、更新；当事务2对该记录进行更新时，事务1再次读取该记录，读到的仍然是第一次读取的那个版本；事务1更新某行记录时，事务2不能对这行记录做更新，直到事务1结束。

SERIALIZABLE 的原理:事务在读取数据时，必须先对其加**表级共享锁，直到事务结束才释放**；事务在更新数据时，必须先对其加**表级排他锁 ，直到事务结束才释放**。
表现：事务1正在读取A表中的记录时，则事务2也能读取A表，但不能对A表做更新、新增、删除，直到事务1结束；事务1正在更新A表中的记录时，则事务2不能读取A表的任意记录，更不可能对A表做更新、新增、删除，直到事务1结束。



RedoLog和BinLog的区别

- redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
- redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
- redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。
- redo log 适用于崩溃恢复(crash-safe)。binlog适用于主从复制和数据恢复。

binlog 日志只用于归档，但仅仅依靠 binlog 是没有 crash-safe 能力的。但只有 redo log 也不行，因为 redo log 是 InnoDB 特有的，且日志记录落盘后会被覆盖掉。因此需要 binlog 和 redo log 二者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失。



> 引用自：https://spongecaptain.cool/post/database/logicalandphicallog/

物理日志和逻辑日志的区别

物理日志与逻辑日志在存储内容上有很大的区别，存储内容是区分它们的最重要手段。

- 物理日志：存储内容，存储数据库中特定记录的变更，通常是 page oriented，即描述具体某一个 page 的修改操作；例子：一条更新请求对应的初始值（original value）以及更新值（after value）；
- 逻辑日志：存储内容，存储事务中的一个操作；例子：事务中的 UPDATE、DELETE 以及 INSERT 操作。
- 幂等性：物理日志能够做的幂等性，因为其本质是对状态机某一个字段在更新前后状态的记录，无论执行多少次，最终得到的状态总是相同的；逻辑日志并不能够提供幂等性的语义，因为某一个更新操作本身不具备幂等性。
- 效率：逻辑日志比物理日志在重放时有着更低的效率


CheckSum算法思想

- CheckSum生成：将发送数据逐个累加，再对累加和取反，该值即为CheckSum校验码。取反的方式在C语言中可以使用按位取反“~”来实现，再Vb.net上可以吃用与0xFF异或(XOR)的方式得到。
- CheckSum校验：将接收到的数据与校验码连续累加，将结果取反，若取反后的值为0，则表示校验正确，数据有效；否则，校验错误，数据无效。
- 如：带传送的数据串为：0x01，0x12，0x55，0xF0，则该校验码为以上累加，累加值等于0x158，在一个字节中对256取模，结果为 0x58，所以校验码为0x58 ^ 0xFF =  0xA7 。在接收端，如果数据接受正确，则 传送数据 + 校验值 = 0x58 + 0xA7 = 0xFF，因此对该值取反的结果为0，因此校验正确。







